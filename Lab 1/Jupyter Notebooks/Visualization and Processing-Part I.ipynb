{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Processing\n",
    "---\n",
    "By: Kris Ghimire, Thad Schwebke, Walter Lai, and Jamie Vo\n",
    "<img src=\"Images/broken-1391025_1280.JPG\" alt=\"Crime\" style=\"width: 80%;\"/>\n",
    "Photo Cred.: Photo by kat wilcox from Pexels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in libraries\n",
    "\n",
    "# general libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# visualizations libraries\n",
    "import seaborn as sns\n",
    "import plotly \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine Learning \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding (10 pts)\n",
    "---\n",
    "\n",
    "#### Purpose of the dataset.\n",
    "[Homocide Data](https://www.kaggle.com/murderaccountability/homicide-reports)\n",
    "\n",
    "(i.e., why was this data collected in\n",
    "the first place?). \n",
    "\n",
    "The Murder Accountability Project is a nonprofit organization that discovers discrepancies between the reported homicides between medical examiners and the FBI voluntary crime report. The database is considered to be one of the most exhaustive record collection of homicides that is currently avaiable for the US. Additional information about the organization can be found at [Murder Accountability Project](http://www.murderdata.org/).\n",
    "\n",
    "The dataset dates back to 1967 and includes demographic information such as gender, age, and ethnicity. A more in depth description of the attributes may be found in the [Data Description](#Data_Description) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "df = pd.read_csv('../Data/database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of records and columns\n",
    "records = len(df)\n",
    "attributes = df.columns\n",
    "\n",
    "print(f'No. of Records: {records} \\nNo. of Attributes: {len(attributes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and measure the dataset outcomes.\n",
    "Describe how you would define and measure the outcomes from the\n",
    "dataset.\n",
    "That is, why is this data important and how do you know if you have mined\n",
    "useful knowledge from the dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Statistics\n",
    "How would you measure the effectiveness of a\n",
    "good prediction algorithm? Be specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding (80 pts total)\n",
    "---\n",
    "<a id=\"Data_Description\"></a>\n",
    "#### [10 points]  Data Description:\n",
    "Describe the meaning and type of data (scale, values, etc.) for each\n",
    "attribute in the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [15 points] Verify data quality: \n",
    "Explain any missing values, duplicate data, and outliers.\n",
    "Are those mistakes? How do you deal with these problems? Be specific.\n",
    "\n",
    "---\n",
    "\n",
    "AGE\n",
    "    - When age is 0, this is considered to be unknown data\n",
    "    - When age is 99+ or 998, this indicates aged greater than 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [10 points] Statistics:\n",
    "Give simple, appropriate statistics (range, mode, mean, median, variance,\n",
    "counts, etc.) for the most important attributes and describe what they mean or if you\n",
    "found something interesting. Note: You can also use data from other sources for\n",
    "comparison. Explain the significance of the statistics run and why they are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic statistics of categorical data\n",
    "df_categorical = df.select_dtypes(include='object')\n",
    "df_categorical.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all levels per categorical attribute\n",
    "df_categorical_levels = pd.DataFrame()\n",
    "df_categorical_levels['Attribute'] = df_categorical.columns\n",
    "df_categorical_levels['Levels'] = ''\n",
    "df_categorical_levels['Levels_Count'] = ''\n",
    "df_categorical_levels['Unknown_Count'] = ''\n",
    "\n",
    "# populate the dataframe with categorical levels and count of each category\n",
    "for i, row in df_categorical_levels.iterrows():\n",
    "    attribute = row['Attribute']\n",
    "    df_categorical_levels.at[i,'Levels'] = df[attribute].unique()\n",
    "    df_categorical_levels.at[i,'Levels_Count'] = len(df[attribute].unique())\n",
    "    try:\n",
    "        df_categorical_levels.at[i,'Unknown_Count'] = df.groupby(attribute).count().loc['Unknown'][0]\n",
    "    except: \n",
    "        df_categorical_levels.at[i,'Unknown_Count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the dataframe\n",
    "df_categorical_levels.sort_values(by='Unknown_Count', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes with the greatest amount of missing data are ethnicity, relationship, and perpetrator race/sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic statistics for continuous variables\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Victim Age').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [15 points] Visualization\n",
    "Visualize the most important attributes appropriately (at least 5 attributes).\n",
    "Important: Provide an interpretation for each chart. Explain for each attribute why the\n",
    "chosen visualization is appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(df[['Year', 'Incident', 'Victim Age', 'Victim Count','Perpetrator Count']])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [15 points] EDA\n",
    "Explore relationships between attributes: Look at the attributes via scatter\n",
    "plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate. Explain\n",
    "any interesting relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/rogerallen/1583593\n",
    "states = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'American Samoa': 'AS',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Guam': 'GU',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhodes Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY'\n",
    "}\n",
    "df_state = df.groupby('State').count().reset_index()\n",
    "\n",
    "df_state['State_Abb'] = [states[full_state] for full_state in df_state['State']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat map of states \n",
    "\n",
    "fig = px.choropleth(locations=df_state['State_Abb'], \n",
    "                    locationmode=\"USA-states\", \n",
    "                    color=df_state['Record ID'], \n",
    "                    color_continuous_scale='portland',\n",
    "                    scope=\"usa\")\n",
    "fig.update_layout(\n",
    "    title_text = 'Homicide Rates per State',\n",
    "    geo_scope='usa', # limite map scope to USA\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [10 points] Discoveries\n",
    "Identify and explain interesting relationships between features and the class\n",
    "you are trying to predict (i.e., relationships with variables and the target classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [5 points] New Feature Creation\n",
    "Are there other features that could be added to the data or created from\n",
    "existing features? Which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummy Code\n",
    "- Dummy code the categorical data\n",
    "- export to csv due to time required for loop to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create dummy variables\n",
    "def dummy_code(col, df): # input the column names and dataframe\n",
    "    df_dummy = pd.DataFrame()\n",
    "    for val in col:\n",
    "        df_dummy_temp = pd.get_dummies(df[val], prefix=val)\n",
    "        df_dummy = pd.concat([df_dummy, df_dummy_temp], axis=1, sort=False)\n",
    "    return df_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns for cummy coding\n",
    "cat_col = df_categorical.columns.values\n",
    "categorical = np.delete(cat_col, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function for dummy coding variables\n",
    "df_dummy = dummy_code(categorical, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below has been commented out to prevent rerunning unless necessary due to computing power required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "#df_full = pd.concat([df_dummy, df[df.describe().columns]], axis=1, sort=False)\n",
    "#df_full = pd.concat([df_dummy, df[['Agency Name', 'Agency Code']]], axis=1, sort=False)\n",
    "#df_full.to_csv('../Data/Dummy_coded_database.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exceptional Work (10 points total)\n",
    "• You have free reign to provide additional analyses.\n",
    "• One idea: implement dimensionality reduction, then visualize and interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA \n",
    "In this example, the data set will be used to determine the probability that a crime will be solved\n",
    "Response Variable: Crime Solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "df_full = pd.read_csv('../Data/Dummy_coded_database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Test Split\n",
    "- Train/Test split due to the large data size and for data validation [Resource](https://data-flair.training/blogs/train-test-set-in-python-ml/#:~:text=%20How%20to%20Split%20Train%20and%20Test%20Set,our%20model%20on%20the%20train%20data...%20More%20)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "random.seed(1234)\n",
    "df_pca = df_full.drop(['Agency Name', 'Agency Code'], axis=1)\n",
    "# split into train/test\n",
    "y = df_pca['Crime Solved_Yes']\n",
    "x = df_pca.drop(['Crime Solved_Yes', 'Crime Solved_No'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA Code\n",
    "---\n",
    "1. Since PCA is sensitive to scales, the first step is to scale the data [Resource](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns=['PCA_'+ str(x) for x in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PCA = pd.concat([principalDf, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(principalComponents, x=df_PCA['PCA_0'], y=df_PCA['PCA_1'], color=df_PCA['Crime Solved_Yes'])\n",
    "fig.update_layout(title='PCA 1 vs. PCA 2',\n",
    "                  yaxis_zeroline=False, xaxis_zeroline=False)\n",
    "fig.update_xaxes(title_text='PCA 1')\n",
    "fig.update_yaxes(title_text='PCA 2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression\n",
    "---\n",
    "\n",
    "###### Balancing the Dataset\n",
    "The data set is skewed heavily to the yes side, as shown in table below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for a balanced dataset\n",
    "df_crime = df_full[['Crime Solved_Yes', 'Crime Solved_No']].groupby('Crime Solved_Yes').count().reset_index().rename(columns={'Crime Solved_No':'Count'})\n",
    "df_crime['Solved'] = ['No', 'Yes']\n",
    "df_crime = df_crime.drop('Crime Solved_Yes', axis=1)\n",
    "total = df_crime['Count'].sum()\n",
    "df_crime['Percentage'] = [x/total for x in df_crime['Count']]\n",
    "df_crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down sampling will be used to balance out the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSDS ML I",
   "language": "python",
   "name": "ml_i_msds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
